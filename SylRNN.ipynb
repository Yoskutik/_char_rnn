{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.nn import weighted_cross_entropy_with_logits\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['\\u200e', '[', ']', '(', ')', '\\x98', '́', '\\r', ';']\n",
    "replace = {\n",
    "    '»': '\"',\n",
    "    '«': '\"',\n",
    "    '“': '\"',\n",
    "    '„': '\"',\n",
    "    '...': '…',\n",
    "    '—': '-',\n",
    "}\n",
    "signs = ['.', ',', '\"', '…', '-', '\\n', '?', '!', ':']\n",
    "vowels = 'а у о ы и э я ю ё е ь'.split(' ')\n",
    "consonants = 'б в г д ж з й к л м н п р с т ф х ц ч ш щ ъ'.split(' ')\n",
    "\n",
    "def preprocess_str(string):\n",
    "    string = string.lower()\n",
    "    for x in remove:\n",
    "        string = string.replace(x, '')\n",
    "    for key, value in replace.items():\n",
    "        string = string.replace(key, value)\n",
    "    string = re.sub(r'[. ]{2,}', '. ', string)\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    return string\n",
    "\n",
    "def split_to_syllables(text):\n",
    "    syllables = []\n",
    "    cur_syl = ''\n",
    "    v_in_cur_syl = False\n",
    "    for l in text:\n",
    "        if l in signs or l == ' ':\n",
    "            syllables.append(cur_syl)\n",
    "            syllables.append(l)\n",
    "            cur_syl = ''\n",
    "            v_in_cur_syl = False\n",
    "        else:\n",
    "            if l in vowels and v_in_cur_syl:\n",
    "                if cur_syl[-1] in consonants:\n",
    "                    syllables.append(cur_syl[:-1])\n",
    "                    cur_syl = cur_syl[-1] + l\n",
    "                else:\n",
    "                    syllables.append(cur_syl)\n",
    "                    cur_syl = l\n",
    "            elif l in vowels and not v_in_cur_syl:\n",
    "                v_in_cur_syl = True\n",
    "                cur_syl += l\n",
    "            else:\n",
    "                cur_syl += l\n",
    "    syllables.append(cur_syl)\n",
    "    return list(filter(lambda x: x, syllables))\n",
    "\n",
    "syllabels = set()\n",
    "all_texts = ''\n",
    "for path in glob.glob('poems/*.txt'):\n",
    "    with open(path, 'rb') as f:\n",
    "        all_texts += preprocess_str(f.read().decode('utf-8'))\n",
    "syllabels.update(split_to_syllables(all_texts))\n",
    "\n",
    "corpus = {value: i for i, value in enumerate(sorted(syllabels))}\n",
    "corpus_inv = {value: key for key, value in corpus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poem(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        text = preprocess_str(f.read().decode('utf-8'))\n",
    "    syllabels = [corpus[x] for x in split_to_syllables(text)]\n",
    "    remains = len(syllabels) % (window_size + 1) / (window_size + 1)\n",
    "    total = len(syllabels) // (window_size + 1)\n",
    "    chunks = [syllabels[i*(window_size + 1):(i+1)*(window_size + 1)] for i in range(total)]\n",
    "    if (len(syllabels) - total) / window_size > 0.2:\n",
    "        chunks.append(syllabels[-window_size-1:])\n",
    "    return chunks\n",
    "\n",
    "def read_all_poems():\n",
    "    chunks = []\n",
    "    for path in glob.glob('poems/*.txt'):\n",
    "        chunks.extend(read_poem(path))\n",
    "    \n",
    "    ds = Dataset.from_tensor_slices(chunks)\n",
    "    return ds.map(lambda x: (x[:-1], x[1:])) \\\n",
    "        .shuffle(8096).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset = read_all_poems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 3072)          8745984   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 2847)          2918175   \n",
      "=================================================================\n",
      "Total params: 28,445,471\n",
      "Trainable params: 28,445,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size):\n",
    "    return Sequential([\n",
    "        Embedding(len(corpus), 3072, batch_input_shape=[batch_size, None]),\n",
    "        LSTM(1024, return_sequences=True, stateful=True),\n",
    "        Dense(len(corpus)),\n",
    "    ])\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = build_model(batch_size)\n",
    "model.compile('adam', loss=loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "126/126 [==============================] - 17s 132ms/step - loss: 4.5068\n",
      "Epoch 2/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 3.7732\n",
      "Epoch 3/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 3.4099\n",
      "Epoch 4/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 3.2134\n",
      "Epoch 5/25\n",
      "126/126 [==============================] - 16s 130ms/step - loss: 3.0767\n",
      "Epoch 6/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 2.9675\n",
      "Epoch 7/25\n",
      "126/126 [==============================] - 17s 131ms/step - loss: 2.8744\n",
      "Epoch 8/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.7897\n",
      "Epoch 9/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.7101\n",
      "Epoch 10/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.6375\n",
      "Epoch 11/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.5643\n",
      "Epoch 12/25\n",
      "126/126 [==============================] - 16s 127ms/step - loss: 2.4928\n",
      "Epoch 13/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 2.4244\n",
      "Epoch 14/25\n",
      "126/126 [==============================] - 16s 130ms/step - loss: 2.3534\n",
      "Epoch 15/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.2832\n",
      "Epoch 16/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 2.2148\n",
      "Epoch 17/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.1423\n",
      "Epoch 18/25\n",
      "126/126 [==============================] - 16s 130ms/step - loss: 2.0751\n",
      "Epoch 19/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 2.0061\n",
      "Epoch 20/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 1.9349\n",
      "Epoch 21/25\n",
      "126/126 [==============================] - 16s 129ms/step - loss: 1.8699\n",
      "Epoch 22/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 1.8007\n",
      "Epoch 23/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 1.7318\n",
      "Epoch 24/25\n",
      "126/126 [==============================] - 16s 127ms/step - loss: 1.6694\n",
      "Epoch 25/25\n",
      "126/126 [==============================] - 16s 128ms/step - loss: 1.6050\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=25,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join('training_checkpoints', 'ckpt_{epoch}'),\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(1)\n",
    "model.load_weights(tf.train.latest_checkpoint('training_checkpoints'))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 500\n",
    "    input_eval = [corpus[s] for s in split_to_syllables(start_string)]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions /= 1.5\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(corpus_inv[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ах, как хочется мне осень!\n",
      "желтый брег на теле бренном\n",
      "венков в дарниском опершись,\n",
      "исполнелась лобзают\n",
      "мерженье крашее угры,\n",
      "к теле? легкою счастлив, смерть, близ мухой да разлукаво дни свободны\n",
      "и слабый битвы сладую тобою,\n",
      "дремучих озенов бедный царстчучестейтый утедел?\n",
      "налей своих узпятных предков\n",
      "потомство нахму богатударный -\n",
      "встанов льстию,\n",
      "ехали бишься кликку к тихий подчас\n",
      "не властным искру\n",
      "восстаньем ветвей, очи гусарских очках надменная пупусть: с морщиной примутиях достигнутый,\n",
      "отца шести равнится?\n",
      "явился.-\n",
      "а скромной ветхой сенним приятность несчастной?\n",
      "ухи!. приник святых щальих нама!\n",
      "тишиной звуклишися,\n",
      "с дьбою что восстанет, как сей строптиной\n",
      "уродной младостью ногаюдого желучен\n",
      "законно молодымиман. -\n",
      "вперив раз клобудет певцу любовицу.\n",
      "\n",
      "хвалы, нуйся, закрывшийся инопрекличность,\n",
      "стыдбыль монахмучим:\n",
      "европа быстрый поузчий\n",
      "вблизи мы вам воскримой нежно!\n",
      "разлугась маратать нет?\n",
      "перед их ужаса прозрачном,\n",
      "храбрый в сердечных эливой генерой,\n",
      "курапу…\"\n",
      "други!\" медленно, волною нам нехотясь!\"\n",
      "везде пылагой три\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"ах, как хочется мне осень!\\nжелтый брег на теле бренном\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
